import json, csv, os, time, numpy as np, pybullet as p
from datetime import datetime
import torch.multiprocessing as mp
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Optional
from dofbot import DofbotEnv, any_self_collision
from scipy.spatial.transform import Rotation
import torch
import shutil

# ---------------- é»˜è®¤é‡çº²è¾¹ç•Œ ----------------
KEYS = ['q1', 'q2', 'q3', 'q4', 'q5',
        'x', 'y', 'z', 'a', 'b', 'c', 'd',
        'roll', 'pitch', 'yaw',
        'nx', 'ny', 'nz',
        'ox', 'oy', 'oz',
        'ax', 'ay', 'az']

KEYS_NORM = (
    ['q1_sin','q1_cos','q2_sin','q2_cos','q3_sin','q3_cos','q4_sin','q4_cos','q5_sin','q5_cos'] +
    ['x','y','z','a','b','c','d']+
    ['roll_sin','roll_cos','pitch_sin','pitch_cos','yaw_sin','yaw_cos'] +
    ['nx', 'ny', 'nz', 'ox', 'oy', 'oz', 'ax', 'ay', 'az']
)

# sin/cos åˆ—èŒƒå›´å›ºå®š [-1,1]ï¼›ä½ç½®/å››å…ƒæ•°ç”¨åŸ MIN/MAX
MIN_NORM = np.array(
    [-1,-1]*5 +                            # 10 ä¸ªè§’åº¦
    [-1,-1,-1] +                           # xyz
    [-1,-1,-1,-1] +                        # quat
    [-1,-1]*3 +                             # 6 ä¸ªè§’åº¦
    [-1] * 9
)
MAX_NORM = np.array(
    [1,1]*5 + [1,1,1] + [1,1,1,1] + [1,1]*3 + [1] * 9
)

MIN_VALS = np.array([-np.pi, 0, 0, 0, 0,
                     -0.2444, -0.3170, -0.1273, -1.0, -1.0, -1.0, -1.0,
                     -np.pi, -np.pi/2, -np.pi,
                     -1.0, -1.0, -1.0,   -1.0, -1.0, -1.0,   -1.0, -1.0,-1.0])
MAX_VALS = np.array([np.pi, np.pi, np.pi, np.pi, np.pi,
                     0.3909, 0.3171, 0.4255, 1.0, 1.0, 1.0, 1.0,
                     np.pi, np.pi/2, np.pi,
                     1.0, 1.0, 1.0,   1.0, 1.0, 1.0,   1.0, 1.0, 1.0])

LOWER_POSE = np.array(MIN_VALS[5:15])
UPPER_POSE = np.array(MAX_VALS[5:15])

# ---------- å¯è°ƒå‚æ•° ----------
CHUNK_SIZE   = 1000          # æ¯å¤šå°‘æ¡åˆ·ä¸€æ¬¡ç›˜
WORK_DIR     = Path('dataset') # é¡¶å±‚ç›®å½•

# ---------- æ•°æ®é›†é‡‡é›†ä»¿çœŸå‡½æ•° ----------
def worker(rank: int, samples_per_worker: int, flush_every: int, run_tag: str):
    """
    æ¯ä¸ª worker ç‹¬äº«ä¸€ä¸ªåˆ†ç‰‡ csvï¼Œè¾¹é‡‡è¾¹å†™ã€‚
    è¿”å› (rank, å®é™…å†™å…¥æ¡æ•°, åˆ†ç‰‡æ–‡ä»¶ç»å¯¹è·¯å¾„)
    """
    chunk_file = WORK_DIR / run_tag / f'chunk_{rank:03d}.csv'
    chunk_file.parent.mkdir(parents=True, exist_ok=True)

    # å¦‚æœåˆ†ç‰‡å·²å­˜åœ¨ï¼Œç›´æ¥ç»Ÿè®¡æ¡æ•°åè¿”å›ï¼ˆç»­é‡‡ï¼‰
    if chunk_file.exists():
        with open(chunk_file, 'r', newline='') as f:
            exist_rows = sum(1 for _ in f) - 1  # å»æ‰è¡¨å¤´
        print(f'[Worker {rank}] å‘ç°å·²æœ‰åˆ†ç‰‡ï¼Œè·³è¿‡é‡‡é›†ï¼Œå·²å­˜åœ¨ {exist_rows} æ¡')
        return rank, exist_rows, str(chunk_file.resolve())

    # å¦åˆ™é‡æ–°é‡‡é›†
    gui_options = f"--window_port={6660 + rank} --width=640 --height=480"
    conn = p.connect(p.DIRECT, options=gui_options)
    env = DofbotEnv(physicsClientId=conn)
    env.reset()

    ll = [-np.pi, 0, 0, 0, -np.pi]
    ul = [np.pi, np.pi, np.pi, np.pi, np.pi]

    buf, written = [], 0
    max_attempts = 100
    with open(chunk_file, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(KEYS)          # è¡¨å¤´
        while written < samples_per_worker:
            joint_angles = [np.random.uniform(lo, hi) for lo, hi in zip(ll, ul)]
            joint_angles_now, _ = env.get_dofbot_jointPoses()
            pos_last, _, euler_last = env.get_dofbot_pose()
            err_joint = np.linalg.norm(np.array(joint_angles) - np.array(joint_angles_now))
            err_pos   = 10.0
            attempts  = 0
            while (err_joint > 0.001 or err_pos > 0.001) and attempts < max_attempts:
                env.dofbot_forward_control(joint_angles, 0.0)
                pos_now, orn_now, euler_now = env.get_dofbot_pose()

                # æŠŠ pos + orn + euler æ‹¼æˆ 9 ç»´å‘é‡
                pose_vec = np.concatenate([pos_now, orn_now, euler_now])

                # # åªè¦æœ‰ä¸€ä¸ªç»´åº¦è¶Šç•Œæˆ–è€…|q|â‰ 1å°±è§¦å‘
                # if ((pose_vec < LOWER_POSE).any() or (pose_vec > UPPER_POSE).any() or abs(np.linalg.norm(orn_now) - 1.0) > 1e-3):
                #     attempts = max_attempts
                #     # print(f"[Worker {rank}][Local {i + 1}] unreachable. Retrying...")
                #     break
                #
                # if any_self_collision(env._dofbot.dofbotUid, safety_margin=0.001):
                #     attempts = max_attempts
                #     # print(f"[Worker {rank}][Local {i + 1}] Collision. Retrying...")
                #     break

                joint_angles_now, _ = env.get_dofbot_jointPoses()
                err_joint = np.linalg.norm(np.array(joint_angles) - np.array(joint_angles_now))
                err_pos   = np.linalg.norm(np.array(pos_now) - np.array(pos_last))
                pos_last  = pos_now
                attempts += 1
            if attempts >= max_attempts:
                # print(f"[Worker {rank}][Local {i + 1}] Collision or unreachable. Retrying...")
                env.reset()
                continue   # ç¢°æ’æˆ–ä¸å¯è¾¾ï¼Œé‡é‡‡
            pos_real, orn_real, euler_real = env.get_dofbot_pose()
            R_mat = Rotation.from_quat(orn_real).as_matrix()
            nx, ny, nz = R_mat[:, 0]
            ox, oy, oz = R_mat[:, 1]
            ax, ay, az = R_mat[:, 2]
            row = list(joint_angles) + list(pos_real) + list(orn_real) + list(euler_real)+[nx, ny, nz, ox, oy, oz, ax, ay, az]
            buf.append(row)
            written += 1
            if len(buf) >= flush_every:
                writer.writerows(buf)
                f.flush()
                buf.clear()
                print(f'[Worker {rank}] å·²å†™å…¥ {written + flush_every}/{samples_per_worker}')
            if written % 1000 == 0:
                print(f"âœ… [Worker {rank}] Collected {written}/{samples_per_worker}")
        # å°¾éƒ¨ä¸è¶³ä¸€ chunk
        if buf:
            writer.writerows(buf)
            f.flush()
    # é€€å‡ºæ—¶è®°å¾—æ–­å¼€
    p.disconnect(conn)
    return rank, written, str(chunk_file.resolve())


def angle_encode(theta):
    return [np.sin(theta), np.cos(theta)]

# ---------- é‡‡é›†Dofbotæ­£è¿åŠ¨å­¦æ•°æ®é›† ----------
def collect_dofbot_dataset(num_envs: int = 2,
                           num_samples: int = 4000,
                           show_gui: bool = False,
                           sleep: float = 0.01,
                           flush_every: int = CHUNK_SIZE):
    """
    é‡‡é›† Dofbot æ­£è¿åŠ¨å­¦æ•°æ®é›†
    è¿”å› (N,15) åŸå§‹é‡çº² ndarrayï¼Œå¹¶è‡ªåŠ¨è½ç›˜
    æµå¼é‡‡é›† 120 ä¸‡æ¡ï¼Œå†…å­˜å ç”¨ < num_envsÃ—flush_everyÃ—å•è¡Œå­—èŠ‚æ•°
    """
    mp.set_start_method('spawn', force=True)
    run_tag = datetime.now().strftime("%Y%m%d_%H%M%S")
    samples_per_worker = (num_samples + num_envs - 1) // num_envs

    # 1. å¯åŠ¨æ‰€æœ‰ workerï¼ˆåŒæ­¥è¿”å›åˆ†ç‰‡ä¿¡æ¯ï¼‰
    with mp.Pool(num_envs) as pool:
        results = [
            pool.apply_async(worker, (r, samples_per_worker, flush_every, run_tag))
            for r in range(num_envs)
        ]
        chunk_info = [r.get() for r in results]  # [(rank, cnt, path), ...]

    # 2. ä¸»è¿›ç¨‹ï¼šå½’å¹¶åˆ†ç‰‡ã€è®¡ç®—çœŸå® min/maxã€å†™æœ€ç»ˆä¸‰ä»¶å¥—
    print('\n[Main] æ‰€æœ‰åˆ†ç‰‡é‡‡é›†å®Œæˆï¼Œå¼€å§‹åˆå¹¶ä¸å½’ä¸€åŒ–...')
    raw_csv = WORK_DIR / run_tag / f'dofbot_fk_{num_samples}_raw.csv'
    norm_csv = WORK_DIR / run_tag / f'dofbot_fk_{num_samples}_norm.csv'
    stats_json = WORK_DIR / run_tag / f'dofbot_fk_{num_samples}_norm_stats.json'

    # 2.1 å…ˆæ‰«æä¸€éæ‹¿åˆ°å…¨å±€ min/maxï¼ˆæµå¼ï¼Œä¸åŠ è½½å…¨é‡ï¼‰
    mins = +np.inf * np.ones(24)
    maxs = -np.inf * np.ones(24)
    total_rows = 0
    for _, cnt, path in chunk_info:
        total_rows += cnt
        with open(path, 'r', newline='') as f:
            rdr = csv.reader(f)
            next(rdr)  # è·³è¿‡è¡¨å¤´
            for row in rdr:
                vals = np.array(row, dtype=np.float64)
                mins = np.minimum(mins, vals)
                maxs = np.maximum(maxs, vals)

    # 2.2 å†™åˆå¹¶åçš„ raw æ–‡ä»¶ï¼ˆæµå¼å¤åˆ¶ï¼Œä¸é¢å¤–å å†…å­˜ï¼‰
    with open(raw_csv, 'w', newline='') as dst:
        writer = csv.writer(dst)
        writer.writerow(KEYS)
        for _, _, path in chunk_info:
            with open(path, 'r', newline='') as src:
                next(src)  # ä¸¢è¡¨å¤´
                shutil.copyfileobj(src, dst)  # æŒ‰å—å¤åˆ¶ï¼Œå†…å­˜å‹å¥½

    # 2.3 å†™ stats
    stats = {k: {'min': float(mins[i]), 'max': float(maxs[i])}
             for i, k in enumerate(KEYS)}
    with open(stats_json, 'w') as f:
        json.dump(stats, f, indent=4)

    # 2.4 ç¬¬äºŒéæµå¼ç”Ÿæˆ norm æ–‡ä»¶ï¼ˆ24 ç»´ï¼‰
    with open(norm_csv, 'w', newline='') as dst:
        writer = csv.writer(dst)
        writer.writerow(KEYS_NORM)
        for _, _, path in chunk_info:
            with open(path, 'r', newline='') as src:
                rdr = csv.reader(src)
                next(rdr)
                for row in rdr:
                    vec = np.array(row, dtype=np.float32)
                    # å½’ä¸€åŒ–é€»è¾‘åŒåŸä»£ç 
                    q_raw = vec[:5]
                    xyz = vec[5:8]
                    quat = vec[8:12]
                    euler = vec[12:15]
                    dir_vec = vec[15:24]
                    sc_joint = np.concatenate([np.sin(q_raw), np.cos(q_raw)])
                    xyz_n = 2 * (xyz - MIN_VALS[5:8]) / (MAX_VALS[5:8] - MIN_VALS[5:8]) - 1
                    sc_euler = np.concatenate([np.sin(euler), np.cos(euler)])
                    dir_n = (dir_vec - MIN_VALS[15:24]) / (MAX_VALS[15:24] - MIN_VALS[15:24]) - 1
                    norm_row = np.hstack([sc_joint, xyz_n, quat, sc_euler, dir_n])
                    writer.writerow(norm_row)

    # 2.5 å¯é€‰ï¼šåˆ é™¤åˆ†ç‰‡æ–‡ä»¶ä»¥èŠ‚çœç£ç›˜
    for _, _, path in chunk_info:
        os.remove(path)

    print(f'\nâœ… æµå¼é‡‡é›†å®Œæˆï¼Œæœ€ç»ˆ {total_rows} æ¡ â†’ {raw_csv.parent}')
    return raw_csv, norm_csv, stats_json

    # # åˆ†å¸ƒè¯Šæ–­
    # print('\nğŸ“Š Distribution (same as original):')
    # for i, k in enumerate(KEYS):
    #     col = dataset_np[:, i]
    #     mean, std, cover = float(col.mean()), float(col.std()), \
    #         (col.max() - col.min()) / (MAX_VALS[i] - MIN_VALS[i] + 1e-8) * 100
    #     print(f' - {k}: mean={mean:.4f} std={std:.4f}  coverage={cover:.2f}%')
    #
    # print('\nâœ… Parallel FK dataset collection done.')
    # return dataset_np

    # mp.set_start_method('spawn', force=True)
    # samples_per_worker = (num_samples + num_envs - 1) // num_envs
    # queue = mp.SimpleQueue()
    # processes = [mp.Process(target=worker, args=(r, queue, samples_per_worker, show_gui, sleep))
    #              for r in range(num_envs)]
    # for p in processes:
    #     p.start()
    # # æ”¶é›†
    # dataset_chunks = []
    # for _ in range(num_envs):
    #     rank, chunk = queue.get()
    #     print(f'[Main] received {chunk.shape[0]} samples from Worker {rank}')
    #     dataset_chunks.append(chunk)
    # for p in processes:
    #     p.join()
    # dataset_np = np.concatenate(dataset_chunks, axis=0)[:num_samples]  # ç²¾ç¡®æˆªæ–­
    #
    # ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    # save_dir = os.path.join('dataset', ts)  # ä¾‹å¦‚ dataset/20250919_095516
    # os.makedirs(save_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
    #
    # prefix = os.path.join(save_dir, f'dofbot_fk_{num_samples}')
    #
    # raw_csv   = prefix + '_raw.csv'
    # norm_csv  = prefix + '_norm.csv'
    # stats_json= prefix + '_norm_stats.json'
    #
    # # raw
    # with open(raw_csv, 'w', newline='') as f:
    #     csv.writer(f).writerow(KEYS)
    #     csv.writer(f).writerows(dataset_np.tolist())
    # print('âœ… Raw saved â†’', raw_csv)
    #
    # # ---------- é‡æ–°è®¡ç®—çœŸå® min/max ----------
    # real_min = dataset_np.min(axis=0)
    # real_max = dataset_np.max(axis=0)
    #
    # # æ›´æ–° MIN_VALS å’Œ MAX_VALSï¼ˆç”¨äºåç»­å½’ä¸€åŒ–ï¼‰
    # MIN_VALS[:] = real_min
    # MAX_VALS[:] = real_max
    #
    # # norm
    # # ---------- 1. å…ˆæ‹†åˆ— ----------
    # q_raw = dataset_np[:, :5]  # 5 joint
    # xyz = dataset_np[:, 5:8]  # 3 pos
    # quat = dataset_np[:, 8:12]  # 4 quat
    # euler = dataset_np[:, 12:15]  # 3 euler
    # # ---------- 2. æ‰€æœ‰è§’åº¦ç»Ÿä¸€ sin/cos ----------
    # sin_cos_joint = np.concatenate([np.sin(q_raw), np.cos(q_raw)], axis=1)
    # sin_cos_euler = np.concatenate([np.sin(euler), np.cos(euler)], axis=1)
    # # ä½ç½® & å››å…ƒæ•° â†’ MinMax [-1,1]ï¼ˆå››å…ƒæ•°å·²å¤©ç„¶åœ¨å†…ï¼Œå¯ä¸å†ç¼©æ”¾ï¼‰
    # xyz_norm = 2 * (xyz - MIN_VALS[5:8]) / (MAX_VALS[5:8] - MIN_VALS[5:8]) - 1
    # quat_norm = quat  # å·²åœ¨ [-1,1]
    #
    # # ---------- 3. æ‹¼æœ€ç»ˆå½’ä¸€åŒ–çŸ©é˜µ ----------
    # norm = np.hstack([sin_cos_joint,  # 10 ç»´  (q1~q5)
    #                   xyz_norm,  # 3 ç»´
    #                   quat_norm,  # 4 ç»´
    #                   sin_cos_euler])  # 6 ç»´  (roll,pitch,yaw)
    #
    # # norm = 2 * (dataset_np - MIN_VALS) / (MAX_VALS - MIN_VALS) - 1
    # with open(norm_csv, 'w', newline='') as f:
    #     csv.writer(f).writerow(KEYS_NORM)
    #     csv.writer(f).writerows(norm.tolist())
    # print('âœ… Norm saved â†’', norm_csv)
    #
    # # stats
    # stats = {k: {'min': float(MIN_VALS[i]), 'max': float(MAX_VALS[i])}
    #          for i, k in enumerate(KEYS)}
    # with open(stats_json, 'w') as f:
    #     json.dump(stats, f, indent=4)
    # print('âœ… Stats saved â†’', stats_json)
    #
    # # åˆ†å¸ƒè¯Šæ–­
    # print('\nğŸ“Š Distribution (same as original):')
    # for i, k in enumerate(KEYS):
    #     col = dataset_np[:, i]
    #     mean, std, cover = float(col.mean()), float(col.std()), \
    #                        (col.max()-col.min())/(MAX_VALS[i]-MIN_VALS[i]+1e-8)*100
    #     print(f' - {k}: mean={mean:.4f} std={std:.4f}  coverage={cover:.2f}%')
    #
    # print('\nâœ… Parallel FK dataset collection done.')
    # return dataset_np

# ---------- å¯è§†åŒ–Dofbotæ•°æ®é›†çš„å·¥ä½œç©ºé—´ ----------
def visualize_workspace(raw_csv: str,
                        save_dir: str = "results/workspace_figs",
                        show: bool = True,
                        save: bool = True,
                        point_size: float = 1.0,
                        alpha: float = 0.3):
    """
    å¯è§†åŒ– Dofbot å·¥ä½œç©ºé—´å¹¶ä¿å­˜å›¾ç‰‡

    å‚æ•°
    ----
    raw_csv : str
        é‡‡é›†ç”Ÿæˆçš„ *_raw.csv è·¯å¾„
    save_dir : str
        å›¾ç‰‡ä¿å­˜ç›®å½•ï¼Œä¸å­˜åœ¨ä¼šè‡ªåŠ¨åˆ›å»º
    show : bool
        æ˜¯å¦å¼¹å‡ºçª—å£
    save : bool
        æ˜¯å¦ä¿å­˜ png
    point_size / alpha : float
        æ•£ç‚¹å¤§å°ä¸é€æ˜åº¦
    """
    # ---------- æ•°æ®å…¥å£ ----------
    # ---- è¯»å– csv ----
    data = np.loadtxt(raw_csv, delimiter=',', skiprows=1)
    with open(raw_csv, 'r', newline='') as f:
        header = f.readline().strip().split(',')
    print(f"\nğŸ“Š æ–‡ä»¶ï¼š{raw_csv}")
    for col_idx, name in enumerate(header):
        print(f"  {name:>8s}:  min={data[:, col_idx].min():+.6f}  "
              f"max={data[:, col_idx].max():+.6f}")

    xyz = data[:, 5:8]  # x y z åˆ—

    # 3. ç»˜å›¾ ----------------------------------------------------------
    fig = plt.figure(figsize=(8, 6))
    ax  = fig.add_subplot(111, projection='3d')
    ax.scatter(xyz[:, 0], xyz[:, 1], xyz[:, 2],
               s=point_size, c='dodgerblue', alpha=alpha)

    ax.set_xlabel('X (m)')
    ax.set_ylabel('Y (m)')
    ax.set_zlabel('Z (m)')
    ax.set_title('Dofbot Reachable Workspace')
    ax.set_box_aspect([1, 1, 1])

    # 4. ä¿å­˜ ----------------------------------------------------------
    if save:
        Path(save_dir).mkdir(parents=True, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        name = Path(raw_csv).stem.replace('_norm', '').replace('_raw', '')
        png_path = os.path.join(save_dir, f"{name}_workspace_{ts}.png")
        fig.savefig(png_path, dpi=300, bbox_inches='tight')
        print(f"âœ… å›¾ç‰‡å·²ä¿å­˜ â†’ {png_path}")

    # 5. æ˜¾ç¤º ----------------------------------------------------------
    # 5. æ˜¾ç¤º + q é€€å‡º --------------------------------------------------
    if show:
        plt.show(block=False)  # éé˜»å¡ï¼Œæ‰èƒ½æ¥äº‹ä»¶

        def _quit(event):
            if event.key.lower() == 'q':
                plt.close(fig)

        fig.canvas.mpl_connect('key_press_event', _quit)

        # 50 ms è½®è¯¢ï¼Œç›´åˆ°çª—å£è¢«å…³é—­
        while plt.fignum_exists(fig.number):
            plt.pause(0.05)
    else:
        plt.close(fig)

    return fig, ax


# normalizer.py


class Normalizer:
    """
    ä¸ train_dofbot_model å®Œå…¨å¯¹é½çš„å½’ä¸€åŒ–å·¥å…·ã€‚
    åªå¤„ç† COLS ä¸­å‡ºç°çš„å­—æ®µï¼Œå…¶ä½™å¿½ç•¥ã€‚
    """
    COLS = ['q1', 'q2', 'q3', 'q4', 'q5',
            'x', 'y', 'z', 'a', 'b', 'c', 'd',
            'roll', 'pitch', 'yaw',
            'nx', 'ny', 'nz', 'ox', 'oy', 'oz', 'ax', 'ay', 'az']

    def __init__(self, stats_path: str):
        """
        stats_path: ç”±æ•°æ®é›†é¢„å¤„ç†é˜¶æ®µç”Ÿæˆçš„ *.json
        """
        with open(stats_path, 'r', encoding='utf-8') as f:
            full_stats = json.load(f)

        # åªä¿ç•™ COLS é‡Œå‡ºç°çš„å­—æ®µ
        self.stats = {k: full_stats[k] for k in self.COLS if k in full_stats}
        self.keys = list(self.stats.keys())          # å›ºå®šé¡ºåº
        self.mins = np.array([self.stats[k]['min'] for k in self.keys], dtype=np.float32)
        self.maxs = np.array([self.stats[k]['max'] for k in self.keys], dtype=np.float32)
        self.ranges = self.maxs - self.mins
        self.ranges[self.ranges == 0] = 1.0          # é¿å…é™¤ 0

    # ---------- NumPy ç‰ˆæœ¬ ----------
    def normalize_cols(self, data: np.ndarray, cols) -> np.ndarray:
        """
        data: (N, len(cols)) çš„åŸå§‹å€¼
        cols: ä¸ data åˆ—åé¡ºåºä¸€è‡´çš„ list
        return: å½’ä¸€åŒ–åçš„ (N, len(cols)) æ•°ç»„
        """
        idx = [self.keys.index(c) for c in cols]
        mins = self.mins[idx]
        ranges = self.ranges[idx]
        return 2.0 * (data - mins) / ranges - 1

    def denormalize_cols(self, data: np.ndarray, cols) -> np.ndarray:
        """
        data: (N, len(cols)) çš„å½’ä¸€åŒ–å€¼
        cols: ä¸ data åˆ—åé¡ºåºä¸€è‡´çš„ list
        return: åå½’ä¸€åŒ–åçš„ (N, len(cols)) æ•°ç»„
        """
        idx = [self.keys.index(c) for c in cols]
        mins = self.mins[idx]
        ranges = self.ranges[idx]
        return data * ranges + (mins + ranges / 2.0)

    # ---------- PyTorch ç‰ˆæœ¬ ----------
    def normalize_cols_tensor(self, data: torch.Tensor, cols) -> torch.Tensor:
        idx = [self.keys.index(c) for c in cols]
        mins = torch.as_tensor(self.mins[idx], device=data.device, dtype=data.dtype)
        ranges = torch.as_tensor(self.ranges[idx], device=data.device, dtype=data.dtype)
        return 2.0 * (data - mins) / ranges - 1

    def denormalize_cols_tensor(self, data: torch.Tensor, cols) -> torch.Tensor:
        idx = [self.keys.index(c) for c in cols]
        mins = torch.as_tensor(self.mins[idx], device=data.device, dtype=data.dtype)
        ranges = torch.as_tensor(self.ranges[idx], device=data.device, dtype=data.dtype)
        return data * ranges + (mins + ranges / 2.0)


if __name__ == '__main__':
    collect_data = collect_dofbot_dataset(num_envs=6, num_samples=600, show_gui=True)
    print('é‡‡é›†å®Œæˆï¼Œå½¢çŠ¶ï¼š', collect_data.shape)
    visualize_workspace(data=collect_data)
